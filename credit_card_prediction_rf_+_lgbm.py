# -*- coding: utf-8 -*-
"""Salinan dari AI Credit Card Prediction RF + LGBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-F6hXsTnNOfaVbfNUUREYM0oNpQK-2jr

RANDOM FOREST
"""

# Import libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd

# Load the data
data = pd.read_csv('/content/sample_data/creditcard.csv')

# Step 1: Data Preparation (Hapus NaN)
data_clean = data.dropna()

# Pisahkan fitur (X) dan target (y)
X = data_clean.drop(columns=['Class'])
y = data_clean['Class']

# Step 2: Lebih agresif dalam undersampling
fraud = data_clean[data_clean['Class'] == 1]
normal = data_clean[data_clean['Class'] == 0].sample(n=len(fraud) * 2, random_state=42)  # Kurangi jumlah data normal lebih drastis

data_balanced = pd.concat([fraud, normal])
X_balanced = data_balanced.drop(columns=['Class'])
y_balanced = data_balanced['Class']

# Step 3: Kurangi jumlah fitur lebih banyak
selected_features = X_balanced.columns[:3]  # Hanya 3 fitur
X_reduced = X_balanced[selected_features]

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced)

# Step 5: Random Forest sangat sederhana
rf_model = RandomForestClassifier(
    n_estimators=5,     # Lebih sedikit pohon
    max_depth=2,        # Batasi kedalaman maksimum
    random_state=42
)
rf_model.fit(X_train, y_train)

# Step 6: Predictions and evaluation
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output results
print(f"Accuracy: {accuracy:.2f}")
print(f"Classification Report:\n{report}")

"""LIGHTGBM"""

# Import libraries
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Load the data
data = pd.read_csv("/content/sample_data/creditcard.csv")

# Step 1: Data Preparation (Hapus NaN)
data_clean = data.dropna()

# Step 2: Undersampling lebih ekstrem (rasio 1:0.5)
fraud = data_clean[data_clean['Class'] == 1]
normal = data_clean[data_clean['Class'] == 0].sample(n=len(fraud) // 2, random_state=42)  # Rasio 1:0.5

# Gabungkan kembali dataset yang sudah di-balance
data_balanced = pd.concat([fraud, normal])

# Step 3: Pilih lebih banyak fitur
selected_features = data_balanced.columns[:5]  # Ambil 5 fitur pertama
X_balanced = data_balanced[selected_features]
y_balanced = data_balanced['Class']

# Step 4: Split data
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)

# Step 5: Inisialisasi model LightGBM yang lebih agresif dan di-tuning
lgb_model = lgb.LGBMClassifier(
    boosting_type='gbdt',
    objective='binary',
    is_unbalance=True,
    n_estimators=200,  # Naikkan jumlah pohon lebih banyak
    max_depth=7,     # Kedalaman lebih dalam
    learning_rate=0.05,  # Stabil tapi tetap cepat belajar
    num_leaves=31,  # Tambah kompleksitas daun
    min_data_in_leaf=10,  # Hindari overfitting
    reg_alpha=0.1,  # Regularisasi L1
    reg_lambda=0.1,  # Regularisasi L2
    subsample=0.8,   # Tambah randomness
    colsample_bytree=0.8,
    random_state=42
)
lgb_model.fit(X_train, y_train)

# Step 6: Prediksi dan evaluasi
y_pred = lgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output hasil evaluasi
print(f"Accuracy: {accuracy:.2f}")
print(f"Classification Report:\n{report}")

"""GBOOST"""

# Import libraries
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd

# Load the data
data = pd.read_csv('/content/sample_data/creditcard.csv')

# Step 1: Data Preparation (Hapus NaN)
data_clean = data.dropna()

# Pisahkan fitur (X) dan target (y)
X = data_clean.drop(columns=['Class'])
y = data_clean['Class']

# Step 2: Undersampling lebih agresif
fraud = data_clean[data_clean['Class'] == 1]
normal = data_clean[data_clean['Class'] == 0].sample(n=len(fraud) * 2, random_state=42)

data_balanced = pd.concat([fraud, normal])
X_balanced = data_balanced.drop(columns=['Class'])
y_balanced = data_balanced['Class']

# Step 3: Kurangi jumlah fitur
selected_features = X_balanced.columns[:3]  # Hanya 3 fitur
X_reduced = X_balanced[selected_features]

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced)

# Step 5: Gradient Boosting sederhana
gboost_model = GradientBoostingClassifier(
    n_estimators=5,     # Jumlah boosting stage
    max_depth=2,        # Batasi kedalaman pohon
    learning_rate=0.1,  # Default learning rate
    random_state=42
)
gboost_model.fit(X_train, y_train)

# Step 6: Predictions and evaluation
y_pred = gboost_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output results
print(f"Accuracy: {accuracy:.2f}")
print(f"Classification Report:\n{report}")

"""XGBOOST"""

# Import libraries
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Load the data
data = pd.read_csv("/content/sample_data/creditcard.csv")

# Step 1: Data Preparation (Hapus NaN)
data_clean = data.dropna()

# Step 2: Undersampling lebih ekstrem (rasio 1:0.5)
fraud = data_clean[data_clean['Class'] == 1]
normal = data_clean[data_clean['Class'] == 0].sample(n=len(fraud) // 2, random_state=42)  # Rasio 1:0.5

# Gabungkan kembali dataset yang sudah di-balance
data_balanced = pd.concat([fraud, normal])

# Step 3: Pilih lebih banyak fitur (contoh: 5 fitur pertama)
selected_features = data_balanced.columns[:3]  # Ambil 5 fitur pertama
X_balanced = data_balanced[selected_features]
y_balanced = data_balanced['Class']

# Step 4: Split data
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)

# Step 5: Inisialisasi model XGBoost
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    n_estimators=200,
    max_depth=7,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    scale_pos_weight=1,  # Karena data sudah di-balance
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Training model
xgb_model.fit(X_train, y_train)

# Step 6: Prediksi dan evaluasi
y_pred = xgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Output hasil evaluasi
print(f"Accuracy XGBoost: {accuracy:.2f}")
print(f"Classification Report XGBoost:\n{report}")

"""PUMA"""

# Import libraries
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Load data
data = pd.read_csv("/content/sample_data/creditcard.csv")
data_clean = data.dropna()

# Undersampling
fraud = data_clean[data_clean['Class'] == 1]
normal = data_clean[data_clean['Class'] == 0].sample(n=len(fraud) // 2, random_state=42)
data_balanced = pd.concat([fraud, normal])

# Fitur dan label
X = data_balanced.drop(columns=['Class'])
y = data_balanced['Class']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Logistic Regression (PUMA-style)
pu_model = LogisticRegression(random_state=42)
pu_model.fit(X_train, y_train)
y_pred = pu_model.predict(X_test)

# Evaluasi
print("\n=== PUMA-style Logistic Regression ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(classification_report(y_test, y_pred))

# Import libraries
import pandas as pd
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from scipy.optimize import minimize
import numpy as np

# Load data
data = pd.read_csv("/content/sample_data/creditcard.csv")
data_clean = data.dropna()

# Undersampling
fraud = data_clean[data_clean['Class'] == 1]
normal = data_clean[data_clean['Class'] == 0].sample(n=len(fraud) // 2, random_state=42)
data_balanced = pd.concat([fraud, normal])

# Fitur dan label
X = data_balanced.drop(columns=['Class'])
y = data_balanced['Class']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42)
rf_model.fit(X_train, y_train)
rf_probs = rf_model.predict_proba(X_test)[:, 1]

# LightGBM
lgb_model = lgb.LGBMClassifier(
    boosting_type='gbdt',
    objective='binary',
    is_unbalance=True,
    n_estimators=200,
    max_depth=7,
    learning_rate=0.05,
    num_leaves=31,
    min_data_in_leaf=10,
    reg_alpha=0.1,
    reg_lambda=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
lgb_model.fit(X_train, y_train)
lgb_probs = lgb_model.predict_proba(X_test)[:, 1]

# PUMA-style Logistic Regression
pu_model = LogisticRegression(random_state=42)
pu_model.fit(X_train, y_train)
pu_probs = pu_model.predict_proba(X_test)[:, 1]

# Hybrid optimization
def hybrid_loss(weights):
    w1, w2, w3 = weights
    hybrid_preds = (w1 * rf_probs) + (w2 * lgb_probs) + (w3 * pu_probs)
    hybrid_preds_final = [1 if p > 0.5 else 0 for p in hybrid_preds]
    return -accuracy_score(y_test, hybrid_preds_final)

# Optimisasi bobot (total 1)
constraints = {'type': 'eq', 'fun': lambda w: w[0] + w[1] + w[2] - 1}
bounds = [(0, 1), (0, 1), (0, 1)]
result = minimize(hybrid_loss, [1/3, 1/3, 1/3], bounds=bounds, constraints=constraints)

# Ambil bobot optimal
best_w1, best_w2, best_w3 = result.x

# Final hybrid prediction
hybrid_probs = (best_w1 * rf_probs) + (best_w2 * lgb_probs) + (best_w3 * pu_probs)
hybrid_preds_final = [1 if p > 0.5 else 0 for p in hybrid_probs]

# Evaluasi
print("\n=== Hybrid RF + LGBM + PUMA ===")
print(f"Optimized Accuracy: {accuracy_score(y_test, hybrid_preds_final):.2f}")
print(f"Best Weights: RF={best_w1:.2f}, LGBM={best_w2:.2f}, PUMA={best_w3:.2f}")
print(classification_report(y_test, hybrid_preds_final))